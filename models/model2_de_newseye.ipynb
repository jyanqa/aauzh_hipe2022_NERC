{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9cSYGb2iQmx6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K     |████████████████████████████████| 346 kB 4.1 MB/s \n","\u001b[K     |████████████████████████████████| 4.2 MB 68.0 MB/s \n","\u001b[K     |████████████████████████████████| 86 kB 5.7 MB/s \n","\u001b[K     |████████████████████████████████| 140 kB 64.9 MB/s \n","\u001b[K     |████████████████████████████████| 86 kB 6.7 MB/s \n","\u001b[K     |████████████████████████████████| 212 kB 65.1 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 53.0 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 56.4 MB/s \n","\u001b[K     |████████████████████████████████| 127 kB 69.6 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 47.5 MB/s \n","\u001b[K     |████████████████████████████████| 94 kB 4.1 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 80.1 MB/s \n","\u001b[K     |████████████████████████████████| 144 kB 56.6 MB/s \n","\u001b[K     |████████████████████████████████| 112 kB 75.2 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n","Requirement already satisfied: numpy\u003e=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n","Requirement already satisfied: pytz\u003e=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n","Requirement already satisfied: python-dateutil\u003e=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003e=2.7.3-\u003epandas) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (6.0.1)\n","Requirement already satisfied: numpy\u003e=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.21.6)\n"]}],"source":["# install huggingface and datasets\n","!pip install -q datasets transformers\n","!pip install torch\n","!pip install pandas\n","!pip install pyarrow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5wXIg9OkQQAc"},"outputs":[],"source":["from datasets import load_dataset, ClassLabel\n","from transformers import BertForTokenClassification, BertTokenizer, TrainingArguments, Trainer\n","import numpy as np\n","import torch\n","from sklearn import metrics\n","\n","\n","import pandas as pd\n","import datasets\n","from datasets.features import ClassLabel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Z0ZLJDMuvuB_"},"outputs":[],"source":["torch.cuda.is_available()\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmL15U4QRxhv"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!ls '/content/drive/MyDrive/e_ML4NLP2/v2.1/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-Dem-oVIGjl"},"outputs":[],"source":["train_path = '/content/drive/MyDrive/e_ML4NLP2/v2.1/newseye/de/HIPE-2022-v2.1-newseye-train-de.tsv'\n","dev_path = '/content/drive/MyDrive/e_ML4NLP2/v2.1/newseye/de/HIPE-2022-v2.1-newseye-dev-de.tsv'\n","test_path = '/content/drive/MyDrive/e_ML4NLP2/v2.1/newseye/de/HIPE-2022-v2.1-newseye-test_allmasked-de.tsv'\n","#model = 'dbmdz/flair-clef-hipe-german-base' #not work\n","#model = 'dbmdz/bert-base-german-europeana-uncased' # not work\n","#model = 'dbmdz/flair-historic-ner-onb' # not work\n","#model = 'bert-base-german-dbmdz-uncased' #work\n","#model = 'xlm-mlm-ende-1024' #multilanguage model engl-de #not try yet\n","model = \"bert-base-german-cased\" #work\n","#model = 'dbmdz/bert-base-german-europeana-cased'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFAq2IPcIi_x"},"outputs":[],"source":["# import dataset from cloned git repo\n","def load_dataset(path):\n","    df = pd.read_csv(path, sep='\\t', skip_blank_lines=False, engine='python', quoting=3)\n","    # error_bad_lines=False, \n","    return df\n","\n","\n","tsv_train = load_dataset(train_path)\n","tsv_dev = load_dataset(dev_path)\n","tsv_test = load_dataset(test_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a987pJwFImiH"},"outputs":[],"source":["def simple_preprocess(dataframe):\n","    # Add end_of_document token in df\n","    dataframe = dataframe.dropna(subset=['TOKEN'])\n","\n","    # Filter out metadata rows beginning with #\n","    dataframe = dataframe[~dataframe['TOKEN'].astype(str).str.startswith('#')]\n","    dataframe = dataframe[~dataframe['TOKEN'].astype(str).str.startswith('\\t')]\n","\n","    #transforming nan var from Float to string to use in (***)\n","    dataframe.MISC = dataframe.MISC.fillna('')\n","\n","    return dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XA-OtfdAIreM"},"outputs":[],"source":["tsv_train = simple_preprocess(tsv_train)\n","tsv_dev = simple_preprocess(tsv_dev)\n","tsv_test = simple_preprocess(tsv_test)\n","#tsv_train = tsv_train.reset_index()\n","tsv_dev.head(100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnwvVMWNI4C7"},"outputs":[],"source":["label_set = tsv_train['NE-COARSE-LIT'].unique()\n","# define the label mapping for NER\n","label_list = label_set.tolist()\n","label_list.append('_')\n","label_num = len(label_list)\n","\n","labels = ClassLabel(num_classes=label_num, names=label_list)\n","\n","labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zr-PJWi7IwPX"},"outputs":[],"source":["def create_huggingface_file(dataframe):\n","    import pyarrow as pa\n","    import re\n","    from datasets import Dataset\n","\n","    #creating dataset in json\n","    hug_out = []\n","    idx = 0 \n","    items = {'id': idx,'words':[ ], 'ner': [ ]}\n","    hug_out.append(items)\n","    for index, row in dataframe.iterrows():\n","        if  not re.search(r'EndOfSentence', row['MISC']):\n","            items['words'].append(row['TOKEN'])\n","            items['ner'].append(labels.str2int(row['NE-COARSE-LIT']))\n","        else:\n","            items['words'].append(row['TOKEN'])\n","            items['ner'].append(labels.str2int(row['NE-COARSE-LIT']))\n","            idx += 1\n","            items = {'id': idx,'words':[ ], 'ner': [ ]}\n","            hug_out.append(items)\n","    #filter hug_out out, delete items which has len(words) \u003e 380\n","    #hug_out = filter(lambda x: len(x['words']) \u003c 380, hug_out)\n","    #json to df\n","    hug_out = pd.DataFrame(hug_out)\n","\n","    # delete all sentences that are too long\n","    #hug_out = hug_out[hug_out['words'].map(len) \u003c 512] #why does not work? QA\n","\n","    ### convert to Huggingface dataset\n","    hug_out = Dataset(pa.Table.from_pandas(hug_out))\n","\n","    return hug_out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0OktnbyIzIO"},"outputs":[],"source":["train = create_huggingface_file(tsv_train)\n","val = create_huggingface_file(tsv_dev)\n","test = create_huggingface_file(tsv_test)\n","\n","#look at training data\n","for i in range(10):\n","  print(train[i])\n","\n","print(len(train))\n","print(len(val))\n","print(len(test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LXfT_ygsZBle"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdfXFDPIabf1"},"outputs":[],"source":["def tokenize_func(batch):\n","    tokenized = tokenizer(batch[\"words\"], is_split_into_words=True, padding=\"max_length\", max_length=100, truncation=True)\n","    tokenized_words = [\n","        [\n","            tokenizer(word, add_special_tokens=False)[\"input_ids\"]\n","            for word in sent\n","        ]\n","        for sent in batch[\"words\"]\n","    ]\n","    tokenized_ner = []\n","\n","    for sent_words, sent_nes in zip(tokenized_words, batch[\"ner\"]):\n","        tokenized_ner.append(\n","            [labels.str2int(\"O\")]  # BOS symbol\n","            + [\n","                ne\n","                for subwords, ne in zip(sent_words, sent_nes)\n","                for _ in range(len(subwords))\n","            ]\n","            + [labels.str2int(\"O\")]  # EOS symbol\n","        )\n","\n","    # Padding with \"O\"\n","    tokenized[\"labels\"] = [(ner + [labels.str2int(\"O\")] * (100 - len(ner)))[:100] for ner in tokenized_ner]\n","    tokenized[\"subwords\"] = tokenized_words\n","    return tokenized\n","\n","def tokenize_nolabel_func(batch):\n","    tokenized = tokenizer(batch[\"words\"], is_split_into_words=True, padding=\"max_length\", max_length=100, truncation=True)\n","    tokenized_words = [\n","        [\n","            tokenizer(word, add_special_tokens=False)[\"input_ids\"]\n","            for word in sent\n","        ]\n","        for sent in batch[\"words\"]\n","    ]\n","\n","    tokenized[\"subwords\"] = tokenized_words\n","    return tokenized\n","\n","#tiny_train_tokenized = tiny_train.map(tokenize_func, batched=True, batch_size=50)\n","#tiny_test_tokenized = tiny_test.map(tokenize_nolabel_func, batched=True, batch_size=50)\n","\n","train_tokenized = train.map(tokenize_func, batched=True, batch_size=50)\n","val_tokenized = val.map(tokenize_func, batched=True, batch_size=50)\n","test_tokenized = test.map(tokenize_nolabel_func, batched=True, batch_size=50)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPNyuvyzjc_Z"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir = \"content/drive/MyDrive/e_ML4NLP2/\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaSyjldGbAIH"},"outputs":[],"source":["model = BertForTokenClassification.from_pretrained(model, num_labels=labels.num_classes)\n","#model = AutoModel.from_pretrained(model_name)\n","trainer = Trainer(model=model, args=training_args, train_dataset=train_tokenized)\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Xrzs_BAd3az"},"outputs":[],"source":["predictions = trainer.predict(test_tokenized)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXCXpYR6eCMf"},"outputs":[],"source":["outfile = open(\"/content/drive/MyDrive/e_ML4NLP2/teamname_bundle4_de_0.tsv\", \"w\")\n","outfile.write(\"TOKEN\\tNE-COARSE-LIT\\tNE-COARSE-METO\\tNE-FINE-LIT\\tNE-FINE-METO\\tNE-FINE-COMP\\tNE-NESTED\\tNEL-LIT\\tNEL-METO\\tMISC\")\n","\n","for pred, tinput in zip(predictions.predictions, test_tokenized):\n","    pred_word_labels = []\n","\n","    try:\n","        sent_len = tinput[\"input_ids\"].index(0)\n","    except ValueError:\n","        sent_len = len(tinput[\"input_ids\"])\n","    # Start at 1 to skip pred for cls token\n","    i = 1\n","\n","    for word in tinput['subwords']:\n","        \n","        word_score = np.zeros_like(pred[0])\n","        eos_reached = False\n","        try:\n","            for subword in word:\n","                word_score += pred[i]\n","                i += 1\n","            label = labels.int2str(int(np.argmax(word_score)))\n","            pred_word_labels.append(label)\n","        except IndexError:\n","            pred_word_labels.append(\"O\")\n","\n","    for token, label in zip(tinput['words'], pred_word_labels):\n","        outfile.write(\"\\n\" + f\"{token}\\t{label}\\t\" + ('\\t'.join(['_'] * 8)))\n","\n","outfile.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7A-LuKod6w4"},"outputs":[],"source":["accuracy = datasets.load_metric(\"accuracy\")\n","f1_metric = datasets.load_metric(\"f1\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    refs = labels.flatten()\n","    predictions = np.argmax(logits, axis=-1)\n","    print(logits.shape)\n","    print(predictions.shape)\n","    pred = predictions.flatten()\n","    return {\n","        \"accuracy\": accuracy.compute(predictions=pred, references=refs)[\"accuracy\"],\n","        \"f1_micro\": f1_metric.compute(predictions=pred, references=refs, average=\"micro\")[\"f1\"],\n","        \"f1_macro\": f1_metric.compute(predictions=pred, references=refs, average=\"macro\")[\"f1\"],\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3BtGk6_rpWa"},"outputs":[],"source":["Trainer(\n","    model=model,\n","    args=training_args,\n","    eval_dataset=val_tokenized,\n","    compute_metrics=compute_metrics,\n",").evaluate()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"model2_de_newseye.ipynb","provenance":[{"file_id":"1x_yuKdIWlsyYYSnGaeEu7skubBLOzGwK","timestamp":1651145899864}],"toc_visible":true,"version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}